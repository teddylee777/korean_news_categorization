{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./data/edu_news.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['category']==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocessing(sentence):\n",
    "    sentence =re.sub('[^가-힣ㄱ-ㅎㅏ-ㅣa-zA-Z]', ' ', sentence)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['content_cleaned'] = df['content'].apply(preprocessing)\n",
    "content = df['content_cleaned'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "counter = CountVectorizer()\n",
    "c = counter.fit_transform(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kakao Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from khaiii import KhaiiiApi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kakao_stemmer = KhaiiiApi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noun_tokenize(sentence):\n",
    "    result = kakao_stemmer.analyze(sentence)\n",
    "    word_tokens = [l.lex for mor in result for l in mor.morphs if l.tag == 'NNG']\n",
    "    ret = ' '.join(word_tokens)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentence):\n",
    "    result = kakao_stemmer.analyze(sentence)\n",
    "    word_tokens = [l.lex for mor in result for l in mor.morphs]\n",
    "    ret = ' '.join(word_tokens)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokenized_noun'] = df['content_cleaned'].apply(noun_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokenized_noun'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokenized'] = df['content_cleaned'].apply(tokenize)\n",
    "df['tokenized'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentence = df['tokenized_noun'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokenized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./data/news_tokenized.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starts from Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./data/news_tokenized.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentence = df['tokenized'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened = \"\"\n",
    "for sentence in tokenized_sentence:\n",
    "    flattened+=(sentence)\n",
    "    \n",
    "flat = flattened.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "word_counter = Counter(flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = word_counter.most_common()\n",
    "word_dict = dict(word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate word2idx, idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = dict()\n",
    "idx2word = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, value in enumerate(word_dict.keys()):\n",
    "    word2idx[value] = idx\n",
    "    idx2word[idx] = value\n",
    "    \n",
    "last_idx = len(word2idx)\n",
    "word2idx['<UNK>'] = last_idx\n",
    "idx2word[last_idx] = '<UNK>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx['학생'], idx2word[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(word2idx), len(idx2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encoding Look Up Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_lookup = torch.eye(len(word2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_lookup[word2idx['학교']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word to idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_idx = [word2idx[x] for x in flat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_idx[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_one_hot = [one_hot_lookup[x] for x in flat_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batch(data, idx=0, batch_size=20, skip_gram=2)\n",
    "    x = []\n",
    "    target = []\n",
    "    max_length = len(data)\n",
    "    for i in range(batch_size):\n",
    "        if (idx + i +) >= max_length:\n",
    "            break\n",
    "        x.append(data[i+idx])\n",
    "        target.append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def generate_input(dataset, num_skips):\n",
    "    random.shuffle(dataset)  # 문장 단위로 셔플한다.\n",
    "\n",
    "    # 일차원 array로 만든다. (window를 돌리기 위해!)\n",
    "    flatten = []\n",
    "    for list_ in dataset:\n",
    "        flatten += list_\n",
    "\n",
    "    # (나는, 그녀를 보았다.) => (i:그녀를, l:나는), (i:그녀를, l:보았다)\n",
    "    data = []\n",
    "    label = []\n",
    "    for idx in range(num_skips, len(flatten)-num_skips):\n",
    "        data.append(flatten[idx])\n",
    "        data.append(flatten[idx])\n",
    "        label.append([flatten[idx-1]])\n",
    "        label.append([flatten[idx+1]])\n",
    "    return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skip gram dataset build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_skipgram_dataset(text):\n",
    "    import random\n",
    "    data = []\n",
    "    for i in range(2, len(text) - 2):\n",
    "        data.append((text[i], text[i-2], 1))\n",
    "        data.append((text[i], text[i-1], 1))\n",
    "        data.append((text[i], text[i+1], 1))\n",
    "        data.append((text[i], text[i+2], 1))\n",
    "        # negative sampling\n",
    "        for _ in range(4):\n",
    "            if random.random() < 0.5 or i >= len(text) - 3:\n",
    "                rand_id = random.randint(0, i-1)\n",
    "            else:\n",
    "                rand_id = random.randint(i+3, len(text)-1)\n",
    "            data.append((text[i], text[rand_id], 0))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skipgram_train = create_skipgram_dataset(flat[:sample_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define SkipGram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, vocab_size, embd_size):\n",
    "        super(SkipGram, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embd_size)\n",
    "    \n",
    "    def forward(self, focus, context):\n",
    "        embed_focus = self.embeddings(focus).view((1, -1))\n",
    "        embed_ctx = self.embeddings(context).view((1, -1))\n",
    "        score = torch.mm(embed_focus, torch.t(embed_ctx))\n",
    "        log_probs = F.logsigmoid(score)\n",
    "    \n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(skipgram_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(word2idx)\n",
    "embd_size = 2\n",
    "\n",
    "model = SkipGram(vocab_size, embd_size)\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "if use_cuda:\n",
    "    model = model.cuda()\n",
    "    print(\"We are using GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.5\n",
    "n_epoch = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    total_loss = .0\n",
    "    idx = 0\n",
    "    for in_w, out_w, target in skipgram_train:\n",
    "        idx+=1\n",
    "        in_w_var = Variable(torch.LongTensor([word2idx[in_w]]))\n",
    "        out_w_var = Variable(torch.LongTensor([word2idx[out_w]]))\n",
    "        target = Variable(torch.FloatTensor([target]))\n",
    "        if use_cuda:\n",
    "            in_w_var = in_w_var.cuda()\n",
    "            out_w_var = out_w_var.cuda()\n",
    "            target = target.cuda()\n",
    "\n",
    "        model.zero_grad()\n",
    "        log_probs = model(in_w_var, out_w_var)\n",
    "        loss = loss_fn(log_probs[0], target)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    print(\"epoch: {}, loss: {:.5f}\".format(epoch, total_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(skipgram_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('====Test SkipGram===')\n",
    "model.eval()\n",
    "\n",
    "correct_ct = 0\n",
    "\n",
    "for in_w, out_w, target in skipgram_train:\n",
    "    in_w_var = Variable(torch.LongTensor([word2idx[in_w]]))\n",
    "    out_w_var = Variable(torch.LongTensor([word2idx[out_w]]))\n",
    "    \n",
    "    if use_cuda:\n",
    "        in_w_var = in_w_var.cuda()\n",
    "        out_w_var = out_w_var.cuda()\n",
    "    \n",
    "    log_probs = model(in_w_var, out_w_var)\n",
    "    _, predicted = torch.max(log_probs, 1)\n",
    "    predicted = predicted[0]\n",
    "    if predicted == target:\n",
    "        correct_ct += 1\n",
    "\n",
    "print('Accuracy: {:.1f}% ({:d}/{:d})'.format(correct_ct/len(skipgram_train)*100, correct_ct, len(skipgram_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS244 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./data/news_tokenized.csv')\n",
    "# tokenized_sentence = df['tokenized'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentence = df['tokenized_noun'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4339"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened = \"\"\n",
    "for sentence in tokenized_sentence:\n",
    "    flattened+=(sentence)\n",
    "corpus = flattened.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "957037"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "750000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_limit = 750000\n",
    "corpus = corpus[:word_limit]\n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "word_counter = Counter(corpus)\n",
    "word_list = word_counter.most_common()\n",
    "word_dict = dict(word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## more than 500 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = [word for word in word_list if word[1] > 200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = dict(w)\n",
    "len(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = dict()\n",
    "idx2word = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, value in enumerate(word_dict.keys()):\n",
    "    word2idx[value] = idx\n",
    "    idx2word[idx] = value\n",
    "    \n",
    "last_idx = len(word2idx)\n",
    "word2idx['<UNK>'] = last_idx\n",
    "idx2word[last_idx] = '<UNK>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27365, 27365)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2idx), len(idx2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are using GPU\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "if use_cuda:\n",
    "    print(\"We are using GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_skipgram_dataset_neg_sampl(text):\n",
    "    import random\n",
    "    data = []\n",
    "    for i in range(2, len(text) - 2):\n",
    "        data.append((text[i], text[i-2], 1))\n",
    "        data.append((text[i], text[i-1], 1))\n",
    "        data.append((text[i], text[i+1], 1))\n",
    "        data.append((text[i], text[i+2], 1))\n",
    "        # negative sampling\n",
    "        for _ in range(4):\n",
    "            if random.random() < 0.5 or i >= len(text) - 3:\n",
    "                rand_id = random.randint(0, i-1)\n",
    "            else:\n",
    "                rand_id = random.randint(i+3, len(text)-1)\n",
    "            data.append((text[i], text[rand_id], 0))\n",
    "    return data\n",
    "\n",
    "def create_skipgram_dataset(text):\n",
    "    import random\n",
    "    data = []\n",
    "    for i in range(2, len(text) - 2):\n",
    "        data.append((text[i], text[i-2]))\n",
    "        data.append((text[i], text[i-1]))\n",
    "        data.append((text[i], text[i+1]))\n",
    "        data.append((text[i], text[i+2]))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "750000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(set(corpus))\n",
    "vocab.append('<UNK>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27365"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = create_skipgram_dataset(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109444"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, word2index):\n",
    "    idxs = list(map(lambda w: word2index[w] if word2index.get(w) is not None else word2index[\"<UNK>\"], seq))\n",
    "    return Variable(LongTensor(idxs))\n",
    "\n",
    "def prepare_word(word, word2index):\n",
    "    return Variable(LongTensor([word2index[word]]) if word2index.get(word) is not None else LongTensor([word2index[\"<UNK>\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBatch(batch_size, train_data):\n",
    "    random.shuffle(train_data)\n",
    "    sindex = 0\n",
    "    eindex = batch_size\n",
    "    while eindex < len(train_data):\n",
    "        batch = train_data[sindex: eindex]\n",
    "        temp = eindex\n",
    "        eindex = eindex + batch_size\n",
    "        sindex = temp\n",
    "        yield batch\n",
    "    \n",
    "    if eindex >= len(train_data):\n",
    "        batch = train_data[sindex:]\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if use_cuda else torch.ByteTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27365"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert string to integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_p = []\n",
    "y_p = []\n",
    "for tr in train_data:\n",
    "    X_p.append(prepare_word(tr[0], word2idx).view(1, -1))\n",
    "    y_p.append(prepare_word(tr[1], word2idx).view(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('축도', '중산')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = list(zip(X_p, y_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109444"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Skipgram(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, projection_dim):\n",
    "        super(Skipgram,self).__init__()\n",
    "        self.embedding_v = nn.Embedding(vocab_size, projection_dim)\n",
    "        self.embedding_u = nn.Embedding(vocab_size, projection_dim)\n",
    "\n",
    "        self.embedding_v.weight.data.uniform_(-1, 1) # init\n",
    "        self.embedding_u.weight.data.uniform_(0, 0) # init\n",
    "        #self.out = nn.Linear(projection_dim,vocab_size)\n",
    "    def forward(self, center_words,target_words, outer_words):\n",
    "        center_embeds = self.embedding_v(center_words) # B x 1 x D\n",
    "        target_embeds = self.embedding_u(target_words) # B x 1 x D\n",
    "        outer_embeds = self.embedding_u(outer_words) # B x V x D\n",
    "        \n",
    "        scores = target_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2) # Bx1xD * BxDx1 => Bx1\n",
    "        norm_scores = outer_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2) # BxVxD * BxDx1 => BxV\n",
    "        \n",
    "        nll = -torch.mean(torch.log(torch.exp(scores)/torch.sum(torch.exp(norm_scores), 1).unsqueeze(1))) # log-softmax\n",
    "        \n",
    "        return nll # negative log likelihood\n",
    "    \n",
    "    def prediction(self, inputs):\n",
    "        embeds = self.embedding_v(inputs)\n",
    "        \n",
    "        return embeds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 2\n",
    "BATCH_SIZE =128\n",
    "EPOCH = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "model = Skipgram(len(word2idx), EMBEDDING_SIZE)\n",
    "if use_cuda:\n",
    "    model = model.cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch : 0, mean_loss : 10.217020\n",
      "batch : 100, mean_loss : 10.217395\n",
      "batch : 200, mean_loss : 10.218100\n",
      "batch : 300, mean_loss : 10.218887\n",
      "batch : 400, mean_loss : 10.219243\n",
      "batch : 500, mean_loss : 10.219417\n",
      "batch : 600, mean_loss : 10.219728\n",
      "batch : 700, mean_loss : 10.219438\n",
      "batch : 800, mean_loss : 10.218013\n",
      "Epoch : 0, mean_loss : 10.217572\n",
      "batch : 0, mean_loss : 10.070097\n",
      "batch : 100, mean_loss : 10.070762\n",
      "batch : 200, mean_loss : 10.066503\n",
      "batch : 300, mean_loss : 10.061027\n",
      "batch : 400, mean_loss : 10.056170\n",
      "batch : 500, mean_loss : 10.052102\n",
      "batch : 600, mean_loss : 10.047091\n",
      "batch : 700, mean_loss : 10.041313\n",
      "batch : 800, mean_loss : 10.035914\n",
      "Epoch : 1, mean_loss : 10.032278\n",
      "batch : 0, mean_loss : 9.718893\n",
      "batch : 100, mean_loss : 9.744351\n",
      "batch : 200, mean_loss : 9.740856\n",
      "batch : 300, mean_loss : 9.742909\n",
      "batch : 400, mean_loss : 9.742336\n",
      "batch : 500, mean_loss : 9.739427\n",
      "batch : 600, mean_loss : 9.739768\n",
      "batch : 700, mean_loss : 9.736789\n",
      "batch : 800, mean_loss : 9.734621\n",
      "Epoch : 2, mean_loss : 9.733009\n",
      "batch : 0, mean_loss : 9.413476\n",
      "batch : 100, mean_loss : 9.430200\n",
      "batch : 200, mean_loss : 9.443158\n",
      "batch : 300, mean_loss : 9.454534\n",
      "batch : 400, mean_loss : 9.463146\n",
      "batch : 500, mean_loss : 9.470836\n",
      "batch : 600, mean_loss : 9.475052\n",
      "batch : 700, mean_loss : 9.480147\n",
      "batch : 800, mean_loss : 9.484194\n",
      "Epoch : 3, mean_loss : 9.484670\n",
      "batch : 0, mean_loss : 9.142061\n",
      "batch : 100, mean_loss : 9.229798\n",
      "batch : 200, mean_loss : 9.245923\n",
      "batch : 300, mean_loss : 9.260401\n",
      "batch : 400, mean_loss : 9.277642\n",
      "batch : 500, mean_loss : 9.288142\n",
      "batch : 600, mean_loss : 9.296841\n",
      "batch : 700, mean_loss : 9.305457\n",
      "batch : 800, mean_loss : 9.312988\n",
      "Epoch : 4, mean_loss : 9.316202\n",
      "batch : 0, mean_loss : 9.141508\n",
      "batch : 100, mean_loss : 9.091215\n",
      "batch : 200, mean_loss : 9.114648\n",
      "batch : 300, mean_loss : 9.136973\n",
      "batch : 400, mean_loss : 9.152641\n",
      "batch : 500, mean_loss : 9.167147\n",
      "batch : 600, mean_loss : 9.178862\n",
      "batch : 700, mean_loss : 9.188783\n",
      "batch : 800, mean_loss : 9.195739\n",
      "Epoch : 5, mean_loss : 9.199868\n",
      "batch : 0, mean_loss : 9.088512\n",
      "batch : 100, mean_loss : 8.984371\n",
      "batch : 200, mean_loss : 9.016022\n",
      "batch : 300, mean_loss : 9.039399\n",
      "batch : 400, mean_loss : 9.057731\n",
      "batch : 500, mean_loss : 9.073136\n",
      "batch : 600, mean_loss : 9.087498\n",
      "batch : 700, mean_loss : 9.100242\n",
      "batch : 800, mean_loss : 9.108341\n",
      "Epoch : 6, mean_loss : 9.113499\n",
      "batch : 0, mean_loss : 8.907489\n",
      "batch : 100, mean_loss : 8.917214\n",
      "batch : 200, mean_loss : 8.945464\n",
      "batch : 300, mean_loss : 8.969040\n",
      "batch : 400, mean_loss : 8.990980\n",
      "batch : 500, mean_loss : 9.008797\n",
      "batch : 600, mean_loss : 9.022738\n",
      "batch : 700, mean_loss : 9.032850\n",
      "batch : 800, mean_loss : 9.041944\n",
      "Epoch : 7, mean_loss : 9.046214\n",
      "batch : 0, mean_loss : 8.885583\n",
      "batch : 100, mean_loss : 8.852996\n",
      "batch : 200, mean_loss : 8.879775\n",
      "batch : 300, mean_loss : 8.907108\n",
      "batch : 400, mean_loss : 8.931186\n",
      "batch : 500, mean_loss : 8.947826\n",
      "batch : 600, mean_loss : 8.963429\n",
      "batch : 700, mean_loss : 8.975867\n",
      "batch : 800, mean_loss : 8.986870\n",
      "Epoch : 8, mean_loss : 8.991845\n",
      "batch : 0, mean_loss : 8.704396\n",
      "batch : 100, mean_loss : 8.809836\n",
      "batch : 200, mean_loss : 8.841347\n",
      "batch : 300, mean_loss : 8.866749\n",
      "batch : 400, mean_loss : 8.888836\n",
      "batch : 500, mean_loss : 8.905255\n",
      "batch : 600, mean_loss : 8.919925\n",
      "batch : 700, mean_loss : 8.932058\n",
      "batch : 800, mean_loss : 8.942794\n",
      "Epoch : 9, mean_loss : 8.947794\n",
      "batch : 0, mean_loss : 8.650051\n",
      "batch : 100, mean_loss : 8.757355\n",
      "batch : 200, mean_loss : 8.798911\n",
      "batch : 300, mean_loss : 8.828507\n",
      "batch : 400, mean_loss : 8.850336\n",
      "batch : 500, mean_loss : 8.868012\n",
      "batch : 600, mean_loss : 8.880688\n",
      "batch : 700, mean_loss : 8.894087\n",
      "batch : 800, mean_loss : 8.904195\n",
      "Epoch : 10, mean_loss : 8.908557\n",
      "batch : 0, mean_loss : 8.617508\n",
      "batch : 100, mean_loss : 8.719173\n",
      "batch : 200, mean_loss : 8.762547\n",
      "batch : 300, mean_loss : 8.789685\n",
      "batch : 400, mean_loss : 8.813141\n",
      "batch : 500, mean_loss : 8.830657\n",
      "batch : 600, mean_loss : 8.846959\n",
      "batch : 700, mean_loss : 8.859891\n",
      "batch : 800, mean_loss : 8.870316\n",
      "Epoch : 11, mean_loss : 8.875556\n",
      "batch : 0, mean_loss : 8.651201\n",
      "batch : 100, mean_loss : 8.696023\n",
      "batch : 200, mean_loss : 8.733845\n",
      "batch : 300, mean_loss : 8.762094\n",
      "batch : 400, mean_loss : 8.785465\n",
      "batch : 500, mean_loss : 8.803852\n",
      "batch : 600, mean_loss : 8.818141\n",
      "batch : 700, mean_loss : 8.830413\n",
      "batch : 800, mean_loss : 8.840948\n",
      "Epoch : 12, mean_loss : 8.845494\n",
      "batch : 0, mean_loss : 8.597916\n",
      "batch : 100, mean_loss : 8.664398\n",
      "batch : 200, mean_loss : 8.711468\n",
      "batch : 300, mean_loss : 8.738220\n",
      "batch : 400, mean_loss : 8.760299\n",
      "batch : 500, mean_loss : 8.779086\n",
      "batch : 600, mean_loss : 8.792970\n",
      "batch : 700, mean_loss : 8.805305\n",
      "batch : 800, mean_loss : 8.816116\n",
      "Epoch : 13, mean_loss : 8.821126\n",
      "batch : 0, mean_loss : 8.542271\n",
      "batch : 100, mean_loss : 8.645008\n",
      "batch : 200, mean_loss : 8.686417\n",
      "batch : 300, mean_loss : 8.714005\n",
      "batch : 400, mean_loss : 8.735891\n",
      "batch : 500, mean_loss : 8.754648\n",
      "batch : 600, mean_loss : 8.768989\n",
      "batch : 700, mean_loss : 8.780329\n",
      "batch : 800, mean_loss : 8.791015\n",
      "Epoch : 14, mean_loss : 8.796500\n",
      "batch : 0, mean_loss : 8.557779\n",
      "batch : 100, mean_loss : 8.620617\n",
      "batch : 200, mean_loss : 8.661516\n",
      "batch : 300, mean_loss : 8.692823\n",
      "batch : 400, mean_loss : 8.713793\n",
      "batch : 500, mean_loss : 8.732009\n",
      "batch : 600, mean_loss : 8.747589\n",
      "batch : 700, mean_loss : 8.760445\n",
      "batch : 800, mean_loss : 8.771456\n",
      "Epoch : 15, mean_loss : 8.775716\n",
      "batch : 0, mean_loss : 8.557831\n",
      "batch : 100, mean_loss : 8.597725\n",
      "batch : 200, mean_loss : 8.641876\n",
      "batch : 300, mean_loss : 8.670651\n",
      "batch : 400, mean_loss : 8.692866\n",
      "batch : 500, mean_loss : 8.709609\n",
      "batch : 600, mean_loss : 8.726710\n",
      "batch : 700, mean_loss : 8.740406\n",
      "batch : 800, mean_loss : 8.751986\n",
      "Epoch : 16, mean_loss : 8.756628\n",
      "batch : 0, mean_loss : 8.517342\n",
      "batch : 100, mean_loss : 8.578990\n",
      "batch : 200, mean_loss : 8.622480\n",
      "batch : 300, mean_loss : 8.655159\n",
      "batch : 400, mean_loss : 8.678649\n",
      "batch : 500, mean_loss : 8.698673\n",
      "batch : 600, mean_loss : 8.713456\n",
      "batch : 700, mean_loss : 8.725267\n",
      "batch : 800, mean_loss : 8.734963\n",
      "Epoch : 17, mean_loss : 8.739523\n",
      "batch : 0, mean_loss : 8.486820\n",
      "batch : 100, mean_loss : 8.577077\n",
      "batch : 200, mean_loss : 8.617928\n",
      "batch : 300, mean_loss : 8.645253\n",
      "batch : 400, mean_loss : 8.667387\n",
      "batch : 500, mean_loss : 8.683062\n",
      "batch : 600, mean_loss : 8.696464\n",
      "batch : 700, mean_loss : 8.709364\n",
      "batch : 800, mean_loss : 8.719992\n",
      "Epoch : 18, mean_loss : 8.724068\n",
      "batch : 0, mean_loss : 8.505596\n",
      "batch : 100, mean_loss : 8.559577\n",
      "batch : 200, mean_loss : 8.594613\n",
      "batch : 300, mean_loss : 8.628698\n",
      "batch : 400, mean_loss : 8.652845\n",
      "batch : 500, mean_loss : 8.669716\n",
      "batch : 600, mean_loss : 8.683482\n",
      "batch : 700, mean_loss : 8.695560\n",
      "batch : 800, mean_loss : 8.704697\n",
      "Epoch : 19, mean_loss : 8.708822\n",
      "batch : 0, mean_loss : 8.444142\n",
      "batch : 100, mean_loss : 8.534765\n",
      "batch : 200, mean_loss : 8.578774\n",
      "batch : 300, mean_loss : 8.617420\n",
      "batch : 400, mean_loss : 8.642723\n",
      "batch : 500, mean_loss : 8.657051\n",
      "batch : 600, mean_loss : 8.670703\n",
      "batch : 700, mean_loss : 8.681890\n",
      "batch : 800, mean_loss : 8.692070\n",
      "Epoch : 20, mean_loss : 8.696081\n",
      "batch : 0, mean_loss : 8.387730\n",
      "batch : 100, mean_loss : 8.530370\n",
      "batch : 200, mean_loss : 8.572368\n",
      "batch : 300, mean_loss : 8.600032\n",
      "batch : 400, mean_loss : 8.625261\n",
      "batch : 500, mean_loss : 8.643583\n",
      "batch : 600, mean_loss : 8.658356\n",
      "batch : 700, mean_loss : 8.669388\n",
      "batch : 800, mean_loss : 8.679312\n",
      "Epoch : 21, mean_loss : 8.684404\n",
      "batch : 0, mean_loss : 8.387831\n",
      "batch : 100, mean_loss : 8.516645\n",
      "batch : 200, mean_loss : 8.557630\n",
      "batch : 300, mean_loss : 8.589027\n",
      "batch : 400, mean_loss : 8.611542\n",
      "batch : 500, mean_loss : 8.628224\n",
      "batch : 600, mean_loss : 8.643307\n",
      "batch : 700, mean_loss : 8.656986\n",
      "batch : 800, mean_loss : 8.666482\n",
      "Epoch : 22, mean_loss : 8.671873\n",
      "batch : 0, mean_loss : 8.554548\n",
      "batch : 100, mean_loss : 8.510914\n",
      "batch : 200, mean_loss : 8.551216\n",
      "batch : 300, mean_loss : 8.583449\n",
      "batch : 400, mean_loss : 8.604230\n",
      "batch : 500, mean_loss : 8.621241\n",
      "batch : 600, mean_loss : 8.635367\n",
      "batch : 700, mean_loss : 8.646722\n",
      "batch : 800, mean_loss : 8.656061\n",
      "Epoch : 23, mean_loss : 8.661133\n",
      "batch : 0, mean_loss : 8.520069\n",
      "batch : 100, mean_loss : 8.501897\n",
      "batch : 200, mean_loss : 8.541250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch : 300, mean_loss : 8.572153\n",
      "batch : 400, mean_loss : 8.593785\n",
      "batch : 500, mean_loss : 8.611284\n",
      "batch : 600, mean_loss : 8.625329\n",
      "batch : 700, mean_loss : 8.635472\n",
      "batch : 800, mean_loss : 8.645738\n",
      "Epoch : 24, mean_loss : 8.650666\n",
      "batch : 0, mean_loss : 8.489092\n",
      "batch : 100, mean_loss : 8.487107\n",
      "batch : 200, mean_loss : 8.530520\n",
      "batch : 300, mean_loss : 8.559452\n",
      "batch : 400, mean_loss : 8.583995\n",
      "batch : 500, mean_loss : 8.600405\n",
      "batch : 600, mean_loss : 8.615820\n",
      "batch : 700, mean_loss : 8.626559\n",
      "batch : 800, mean_loss : 8.637272\n",
      "Epoch : 25, mean_loss : 8.641920\n",
      "batch : 0, mean_loss : 8.559797\n",
      "batch : 100, mean_loss : 8.481048\n",
      "batch : 200, mean_loss : 8.518001\n",
      "batch : 300, mean_loss : 8.549936\n",
      "batch : 400, mean_loss : 8.572837\n",
      "batch : 500, mean_loss : 8.590425\n",
      "batch : 600, mean_loss : 8.604553\n",
      "batch : 700, mean_loss : 8.616244\n",
      "batch : 800, mean_loss : 8.626697\n",
      "Epoch : 26, mean_loss : 8.631489\n",
      "batch : 0, mean_loss : 8.409271\n",
      "batch : 100, mean_loss : 8.462296\n",
      "batch : 200, mean_loss : 8.512537\n",
      "batch : 300, mean_loss : 8.541052\n",
      "batch : 400, mean_loss : 8.563740\n",
      "batch : 500, mean_loss : 8.584173\n",
      "batch : 600, mean_loss : 8.598121\n",
      "batch : 700, mean_loss : 8.609472\n",
      "batch : 800, mean_loss : 8.619718\n",
      "Epoch : 27, mean_loss : 8.623940\n",
      "batch : 0, mean_loss : 8.314735\n",
      "batch : 100, mean_loss : 8.458787\n",
      "batch : 200, mean_loss : 8.505913\n",
      "batch : 300, mean_loss : 8.538301\n",
      "batch : 400, mean_loss : 8.560061\n",
      "batch : 500, mean_loss : 8.576602\n",
      "batch : 600, mean_loss : 8.591751\n",
      "batch : 700, mean_loss : 8.602610\n",
      "batch : 800, mean_loss : 8.611829\n",
      "Epoch : 28, mean_loss : 8.615061\n",
      "batch : 0, mean_loss : 8.329330\n",
      "batch : 100, mean_loss : 8.446557\n",
      "batch : 200, mean_loss : 8.495415\n",
      "batch : 300, mean_loss : 8.528327\n",
      "batch : 400, mean_loss : 8.551287\n",
      "batch : 500, mean_loss : 8.567708\n",
      "batch : 600, mean_loss : 8.583068\n",
      "batch : 700, mean_loss : 8.594505\n",
      "batch : 800, mean_loss : 8.602690\n",
      "Epoch : 29, mean_loss : 8.606591\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCH):\n",
    "    for i, batch in enumerate(getBatch(BATCH_SIZE, train_data)):\n",
    "        \n",
    "        inputs, targets = zip(*batch)\n",
    "        \n",
    "        inputs = torch.cat(inputs) # B x 1\n",
    "        targets = torch.cat(targets) # B x 1\n",
    "        vocabs = prepare_sequence(list(vocab), word2idx).expand(inputs.size(0), len(vocab))  # B x V\n",
    "        model.zero_grad()\n",
    "\n",
    "        loss = model(inputs, targets, vocabs)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "   \n",
    "        losses.append(loss.item())\n",
    "        if i % 100 == 0:\n",
    "            print(\"batch : %d, mean_loss : %.6f\" % (i, np.mean(losses)))\n",
    "\n",
    "    if epoch % 1 == 0:\n",
    "        print(\"Epoch : %d, mean_loss : %.6f\" % (epoch, np.mean(losses)))\n",
    "        losses = []\n",
    "        torch.save(model.state_dict(), './model/skipgram-embed2-batch128-epoch30')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Skipgram:\n\tsize mismatch for embedding_v.weight: copying a param with shape torch.Size([27365, 200]) from checkpoint, the shape in current model is torch.Size([27365, 2]).\n\tsize mismatch for embedding_u.weight: copying a param with shape torch.Size([27365, 200]) from checkpoint, the shape in current model is torch.Size([27365, 2]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-53529ceb10cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./model/skipgram-001'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/news/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m    767\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m--> 769\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m    770\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_named_members\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_members_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Skipgram:\n\tsize mismatch for embedding_v.weight: copying a param with shape torch.Size([27365, 200]) from checkpoint, the shape in current model is torch.Size([27365, 2]).\n\tsize mismatch for embedding_u.weight: copying a param with shape torch.Size([27365, 200]) from checkpoint, the shape in current model is torch.Size([27365, 2])."
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('./model/skipgram-001'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_2d_graph(vocab):\n",
    "    word = []\n",
    "    vect = []\n",
    "    for v in vocab:\n",
    "        if use_cuda:\n",
    "            vector = model.prediction(prepare_word(v, word2idx))\n",
    "            word.append(v)\n",
    "            vect.append(*vector.data.cpu().numpy())\n",
    "    return word, vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_similarity(target, vocab):\n",
    "    if use_cuda:\n",
    "        target_V = model.prediction(prepare_word(target, word2idx))\n",
    "    else:\n",
    "        target_V = model.prediction(prepare_word(target, word2idx))\n",
    "    similarities = []\n",
    "    for i in range(len(vocab)):\n",
    "        if vocab[i] == target: continue\n",
    "        \n",
    "        if use_cuda:\n",
    "            vector = model.prediction(prepare_word(list(vocab)[i], word2idx))\n",
    "        else:\n",
    "            vector = model.prediction(prepare_word(list(vocab)[i], word2idx))\n",
    "        cosine_sim = F.cosine_similarity(target_V, vector).data.tolist()[0] \n",
    "        similarities.append([vocab[i], cosine_sim])\n",
    "    return sorted(similarities, key=lambda x: x[1], reverse=True)[:10] # sort by similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = random.choice(list(vocab))\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_similarity(\"유치원\", vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_word = []\n",
    "for i in word_counter.most_common(300):\n",
    "    common_word.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word, vect = convert_2d_graph(common_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word[0], vect[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save DataFrame to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = pd.DataFrame(vect, index=word, columns=['x', 'y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# points.to_csv('points.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize w/ Matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.font_manager as fm\n",
    "from matplotlib import rc\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "font_list = fm.findSystemFonts(fontpaths=None, fontext='ttf')\n",
    "nanum_list = [f for f in font_list if 'Nanum' in f]\n",
    "a = fm.FontProperties(fname=nanum_list[0])\n",
    "a.get_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc('font', family=a.get_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig.set_size_inches(40, 20)\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.scatter(points['x'], points['y'])\n",
    "\n",
    "for word, pos in points.iterrows():\n",
    "    ax.annotate(word, pos, fontsize=30)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
