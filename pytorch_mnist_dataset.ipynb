{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_root = './MNIST_DATASET'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data with mean=0.5, std=1.0\n",
    "mnist_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MNIST(download_root, transform=mnist_transform, train=True, download=True)\n",
    "test_dataset = MNIST(download_root, transform=mnist_transform, train=False, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_dataset, \n",
    "                         batch_size=20,\n",
    "                         shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset, \n",
    "                         batch_size=20,\n",
    "                         shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 5, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 5, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(5*5*64, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 5*5*64)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we are using GPU!\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "if use_cuda:\n",
    "    print(\"we are using GPU!\")\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criterion 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 0.173111\n",
      "Train Epoch: 0 [20000/60000 (33%)]\tLoss: 0.150821\n",
      "Train Epoch: 0 [40000/60000 (67%)]\tLoss: 0.305165\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.073801\n",
      "Train Epoch: 1 [20000/60000 (33%)]\tLoss: 0.126338\n",
      "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 0.101409\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.622615\n",
      "Train Epoch: 2 [20000/60000 (33%)]\tLoss: 0.188549\n",
      "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 0.043731\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.013128\n",
      "Train Epoch: 3 [20000/60000 (33%)]\tLoss: 0.149328\n",
      "Train Epoch: 3 [40000/60000 (67%)]\tLoss: 0.043292\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.514427\n",
      "Train Epoch: 4 [20000/60000 (33%)]\tLoss: 0.551940\n",
      "Train Epoch: 4 [40000/60000 (67%)]\tLoss: 0.005374\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.216681\n",
      "Train Epoch: 5 [20000/60000 (33%)]\tLoss: 0.113799\n",
      "Train Epoch: 5 [40000/60000 (67%)]\tLoss: 0.419418\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.027641\n",
      "Train Epoch: 6 [20000/60000 (33%)]\tLoss: 1.215956\n",
      "Train Epoch: 6 [40000/60000 (67%)]\tLoss: 0.611592\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.452951\n",
      "Train Epoch: 7 [20000/60000 (33%)]\tLoss: 0.765688\n",
      "Train Epoch: 7 [40000/60000 (67%)]\tLoss: 0.137372\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.006793\n",
      "Train Epoch: 8 [20000/60000 (33%)]\tLoss: 0.287476\n",
      "Train Epoch: 8 [40000/60000 (67%)]\tLoss: 0.292309\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.719018\n",
      "Train Epoch: 9 [20000/60000 (33%)]\tLoss: 0.177741\n",
      "Train Epoch: 9 [40000/60000 (67%)]\tLoss: 0.548562\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.011252\n",
      "Train Epoch: 10 [20000/60000 (33%)]\tLoss: 0.428870\n",
      "Train Epoch: 10 [40000/60000 (67%)]\tLoss: 0.653453\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.448045\n",
      "Train Epoch: 11 [20000/60000 (33%)]\tLoss: 0.071516\n",
      "Train Epoch: 11 [40000/60000 (67%)]\tLoss: 0.029363\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.429798\n",
      "Train Epoch: 12 [20000/60000 (33%)]\tLoss: 0.113263\n",
      "Train Epoch: 12 [40000/60000 (67%)]\tLoss: 0.227928\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.643293\n",
      "Train Epoch: 13 [20000/60000 (33%)]\tLoss: 0.168188\n",
      "Train Epoch: 13 [40000/60000 (67%)]\tLoss: 0.078590\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.043869\n",
      "Train Epoch: 14 [20000/60000 (33%)]\tLoss: 0.747230\n",
      "Train Epoch: 14 [40000/60000 (67%)]\tLoss: 0.138741\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.123974\n",
      "Train Epoch: 15 [20000/60000 (33%)]\tLoss: 0.158754\n",
      "Train Epoch: 15 [40000/60000 (67%)]\tLoss: 0.478926\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: 0.711993\n",
      "Train Epoch: 16 [20000/60000 (33%)]\tLoss: 0.860222\n",
      "Train Epoch: 16 [40000/60000 (67%)]\tLoss: 0.078455\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss: 0.301657\n",
      "Train Epoch: 17 [20000/60000 (33%)]\tLoss: 0.513629\n",
      "Train Epoch: 17 [40000/60000 (67%)]\tLoss: 0.256951\n",
      "Train Epoch: 18 [0/60000 (0%)]\tLoss: 0.189511\n",
      "Train Epoch: 18 [20000/60000 (33%)]\tLoss: 0.644850\n",
      "Train Epoch: 18 [40000/60000 (67%)]\tLoss: 0.015944\n",
      "Train Epoch: 19 [0/60000 (0%)]\tLoss: 0.107299\n",
      "Train Epoch: 19 [20000/60000 (33%)]\tLoss: 0.153960\n",
      "Train Epoch: 19 [40000/60000 (67%)]\tLoss: 0.475340\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "for epoch in range(20):\n",
    "    avg_loss = 0\n",
    "    for batch_idx, (x, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        if use_cuda:\n",
    "            x, target = x.cuda(), target.cuda()\n",
    "        output = model(x)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 1000 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(x), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 18.1590, Accuracy: 1007/10000 (10%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        if use_cuda:\n",
    "            x, target = x.cuda(), target.cuda()\n",
    "        output = model(x)\n",
    "        test_loss += criterion(output, target, reduction='sum').item() # sum up batch loss\n",
    "        pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "\n",
    "print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_loader.dataset),\n",
    "    100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
